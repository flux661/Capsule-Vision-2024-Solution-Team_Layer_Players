# -*- coding: utf-8 -*-
"""Resnet101SE_result_generation_val_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k-D4Qa3VmCbDXUOUVwC-NsecdImvpqQE
"""

from google.colab import drive
drive.mount('/content/drive')



"""# For validation data"""

!unzip /content/drive/MyDrive/Dataset.zip -d /content/

# How many images in each folder
import os

# Walk through the data
for dirpath,dirnames,filenames in os.walk("Dataset"):
  print(f"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}.")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import os
from collections import Counter
from sklearn.utils import shuffle
from tensorflow.keras.utils import Sequence

class BalancedImageDataGenerator:
    def __init__(self, target_samples=5000, balance_classes=True, **kwargs):
        self.target_samples = target_samples
        self.balance_classes = balance_classes
        self.image_data_generator = ImageDataGenerator(**kwargs)

    def flow_from_directory(self, directory, shuffle=True, **kwargs):
        # First, get the base generator
        base_generator = self.image_data_generator.flow_from_directory(
            directory,
            shuffle=False,  # We'll handle shuffling ourselves
            **kwargs
        )

        # Get all filenames and their corresponding labels
        filenames = base_generator.filenames
        labels = base_generator.classes
        class_indices = base_generator.class_indices
        n_classes = len(class_indices)

        if not self.balance_classes:
            # If not balancing, just use original filenames and labels
            balanced_filenames = filenames
            balanced_labels = labels
        else:
            # Create balanced dataset
            balanced_filenames = []
            balanced_labels = []

            # Count samples per class
            class_counts = Counter(labels)

            for class_idx in range(n_classes):
                class_files = [f for f, l in zip(filenames, labels) if l == class_idx]
                class_count = len(class_files)

                if class_count >= self.target_samples:
                    # Undersample
                    selected_files = shuffle(class_files)[:self.target_samples] if shuffle else class_files[:self.target_samples]
                else:
                    # Oversample
                    multiplier = self.target_samples // class_count
                    remainder = self.target_samples % class_count
                    if shuffle:
                        selected_files = class_files * multiplier + shuffle(class_files)[:remainder]
                    else:
                        selected_files = class_files * multiplier + class_files[:remainder]

                balanced_filenames.extend(selected_files)
                balanced_labels.extend([class_idx] * len(selected_files))

        # Shuffle the dataset if requested
        if shuffle:
            balanced_filenames, balanced_labels = shuffle(balanced_filenames, balanced_labels)

        class DataGenerator(Sequence):
            def __init__(self, parent, filenames, labels, directory, shuffle=True, **kwargs):
                self.parent = parent
                self.filenames = filenames
                self.labels = labels
                self.directory = directory
                self.n = len(filenames)
                self.batch_size = kwargs.get('batch_size', 32)
                self.target_size = kwargs.get('target_size', (224, 224))
                self.shuffle = shuffle
                self.indices = np.arange(self.n)
                self.class_indices = class_indices

                if self.shuffle:
                    np.random.shuffle(self.indices)

            def __len__(self):
                return int(np.ceil(self.n / float(self.batch_size)))

            def __getitem__(self, idx):
                start_idx = idx * self.batch_size
                end_idx = min((idx + 1) * self.batch_size, self.n)
                batch_indices = self.indices[start_idx:end_idx]

                batch_files = [os.path.join(self.directory, self.filenames[i]) for i in batch_indices]
                batch_labels = [self.labels[i] for i in batch_indices]
                batch_image_names = [os.path.basename(self.filenames[i]) for i in batch_indices]

                # Load and preprocess images
                batch_images = []
                for f in batch_files:
                    # Load image
                    img = tf.keras.utils.load_img(f, target_size=self.target_size)
                    # Convert to array
                    img_array = tf.keras.utils.img_to_array(img)
                    # Apply standardization
                    img_array = self.parent.image_data_generator.standardize(img_array)
                    # Apply random transformations
                    img_array = self.parent.image_data_generator.random_transform(img_array)
                    batch_images.append(img_array)

                batch_images = np.array(batch_images)

                # Convert labels to categorical
                batch_labels = tf.keras.utils.to_categorical(batch_labels, n_classes)

                return batch_images, batch_labels, np.array(batch_image_names)

            def on_epoch_end(self):
                if self.shuffle:
                    np.random.shuffle(self.indices)

        return DataGenerator(self, balanced_filenames, balanced_labels, directory, shuffle=shuffle, **kwargs)

# Validation generator without balancing
val_datagen = BalancedImageDataGenerator(
    balance_classes=False,  # Disable balancing for validation
    rescale=1./255
)

val_dir = "/content/Dataset/validation"

validation_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    shuffle=False,
    batch_size=32,
    class_mode='categorical'
)

!unzip /content/drive/MyDrive/resnet_modified_Kaist-20241023T182201Z-001.zip -d /content/



base_model = tf.keras.layers.TFSMLayer('/content/resnet_modified_Kaist', call_endpoint='serving_default')
inputs = tf.keras.Input(shape=(224, 224, 3))  # Set the correct input shape
outputs = base_model(inputs)  # o is your TFSMLayer
resnet_model = tf.keras.Model(inputs, outputs)



import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, roc_auc_score
import json
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, recall_score, f1_score, balanced_accuracy_score

def save_predictions_to_excel(image_paths, y_pred, output_path):
    class_columns = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']
    y_pred_classes = np.argmax(y_pred, axis=1)
    predicted_class_names = [class_columns[i] for i in y_pred_classes]
    df_prob = pd.DataFrame(y_pred, columns=class_columns)
    df_prob.insert(0, 'image_path', image_paths)
    df_class = pd.DataFrame({'image_path': image_paths, 'predicted_class': predicted_class_names})
    df_merged = pd.merge(df_prob, df_class, on='image_path')
    df_merged.to_excel(output_path, index=False)


def calculate_specificity(y_true, y_pred):
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    return specificity

def generate_metrics_report(y_true, y_pred):
    class_columns = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']
    metrics_report = {}

    y_true_classes = np.argmax(y_true, axis=1)
    y_pred_classes = np.argmax(y_pred, axis=1)

    class_report = classification_report(y_true_classes, y_pred_classes, target_names=class_columns, output_dict=True, zero_division=0)

    auc_roc_scores = {}
    for i, class_name in enumerate(class_columns):
        try:
            auc_roc_scores[class_name] = roc_auc_score(y_true[:, i], y_pred[:, i])
        except ValueError:
            auc_roc_scores[class_name] = 0.0

    mean_auc_roc = np.mean(list(auc_roc_scores.values()))
    auc_roc_scores['mean_auc'] = mean_auc_roc

    specificity_scores = {}
    for i, class_name in enumerate(class_columns):
        specificity_scores[class_name] = calculate_specificity(y_true[:, i], (y_pred[:, i] >= 0.5).astype(int))  # Thresholding y_pred

    mean_specificity = np.mean(list(specificity_scores.values()))
    specificity_scores['mean_specificity'] = mean_specificity

    average_precision_scores = {}
    for i, class_name in enumerate(class_columns):
        try:
            precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred[:, i])
            average_precision_scores[class_name] = auc(recall, precision)
        except ValueError:
            average_precision_scores[class_name] = 0.0

    mean_average_precision = np.mean(list(average_precision_scores.values()))
    average_precision_scores['mean_average_precision'] = mean_average_precision

    sensitivity_scores = {}
    for i, class_name in enumerate(class_columns):
        try:
            sensitivity_scores[class_name] = recall_score(y_true[:, i], (y_pred[:, i] >= 0.5).astype(int), zero_division=0)
        except ValueError:
            sensitivity_scores[class_name] = 0.0

    mean_sensitivity = np.mean(list(sensitivity_scores.values()))
    sensitivity_scores['mean_sensitivity'] = mean_sensitivity

    f1_scores = {}
    for i, class_name in enumerate(class_columns):
        try:
            f1_scores[class_name] = f1_score(y_true[:, i], (y_pred[:, i] >= 0.5).astype(int), zero_division=0)
        except ValueError:
            f1_scores[class_name] = 0.0

    mean_f1_score = np.mean(list(f1_scores.values()))
    f1_scores['mean_f1_score'] = mean_f1_score
    balanced_accuracy_scores = balanced_accuracy_score(y_true_classes, y_pred_classes)

    metrics_report.update(class_report)
    metrics_report['auc_roc_scores'] = auc_roc_scores
    metrics_report['specificity_scores'] = specificity_scores
    metrics_report['average_precision_scores'] = average_precision_scores
    metrics_report['sensitivity_scores'] = sensitivity_scores
    metrics_report['f1_scores'] = f1_scores
    metrics_report['mean_auc'] = mean_auc_roc
    metrics_report['mean_specificity'] = mean_specificity
    metrics_report['mean_average_precision'] = mean_average_precision
    metrics_report['mean_sensitivity'] = mean_sensitivity
    metrics_report['mean_f1_score'] = mean_f1_score
    metrics_report['balanced_accuracy'] = balanced_accuracy_scores

    metrics_report_json = json.dumps(metrics_report, indent=4)
    return metrics_report_json

len(validation_generator)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import balanced_accuracy_score, f1_score

# Calculate total number of batches from the generator
total_batches = len(validation_generator)
print(f"Total number of batches: {total_batches}")

# Step 1: Generate predictions for all batches
val_pred = []
val_true = []
val_predictions = []
val_true_classes = []
val_image_names = []

for i, (x, y, img_names) in enumerate(validation_generator):
    if i % 50 == 0:  # Print progress every 50 batches
        print(f"Processing batch {i}/{total_batches}")

    preds = resnet_model.predict(x)
    val_pred.append(preds['output_0'])
    val_predictions.append(np.argmax(preds['output_0'], axis=1))
    val_true.append(y)
    val_true_classes.append(np.argmax(y, axis=1))
    val_image_names.append(img_names)

    if i >= total_batches - 1:
        break

# Step 2: Concatenate predictions, true labels, and image names across batches
val_pred_concateneated = np.concatenate(val_pred)
val_true = np.concatenate(val_true)
val_pred_classes = np.concatenate(val_predictions)
val_true_classes = np.concatenate(val_true_classes)
val_image_names = np.concatenate(val_image_names)

print("\nFinal shapes:",
      "\nTrue classes:", val_true_classes.shape,
      "\nPredicted classes:", val_pred_classes.shape,
      "\nImage names:", val_image_names.shape,
      "\nVal true:", val_true.shape,
      "\nVal_pred_concatenated:", val_pred_concateneated.shape)

# Create a results dictionary mapping image names to their predictions and true labels
results = {
    'image_names': val_image_names,
    'true_classes': val_true_classes,
    'predicted_classes': val_pred_classes
}

# Create a DataFrame for easier analysis
import pandas as pd
results_df = pd.DataFrame({
    'image_name': val_image_names,
    'true_class': val_true_classes,
    'predicted_class': val_pred_classes,
    'correct_prediction': val_true_classes == val_pred_classes
})

# Print some example results
print("\nExample results:")
print(results_df.head())

# Calculate metrics
balanced_acc = balanced_accuracy_score(val_true_classes, val_pred_classes)
f1 = f1_score(val_true_classes, val_pred_classes, average='weighted')

print(f"\nBalanced Accuracy: {balanced_acc:.4f}")
print(f"F1 Score: {f1:.4f}")

# Create confusion matrix
cm = confusion_matrix(val_true_classes, val_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

# Save results to CSV
results_df.to_csv('validation_results.csv', index=False)



generate_metrics_report(val_true,val_pred_concateneated)

def save_predictions_to_excel(image_paths, y_pred, output_path):
    # Ensure output path has .xlsx extension
    if not output_path.endswith('.xlsx'):
        output_path = output_path + '.xlsx'

    # Define class columns
    class_columns = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body',
                    'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']

    # Convert image_paths to list if it's numpy array
    image_paths = image_paths.tolist() if isinstance(image_paths, np.ndarray) else image_paths

    # Get predicted class indices
    y_pred_classes = np.argmax(y_pred, axis=1)

    # Get predicted class names
    predicted_class_names = [class_columns[i] for i in y_pred_classes]

    # Create DataFrame with probabilities
    df_prob = pd.DataFrame(y_pred, columns=class_columns)
    df_prob.insert(0, 'image_path', image_paths)

    # Create DataFrame with predicted classes
    df_class = pd.DataFrame({
        'image_path': image_paths,
        'predicted_class': predicted_class_names
    })

    # Merge DataFrames
    df_merged = pd.merge(df_prob, df_class, on='image_path')

    try:
        # Save to Excel
        df_merged.to_excel(output_path, index=False)
        print(f"Successfully saved predictions to {output_path}")
        print(f"Total images processed: {len(image_paths)}")
        print(f"Shape of final DataFrame: {df_merged.shape}")
    except Exception as e:
        print(f"Error saving to Excel: {str(e)}")

    return df_merged  # Return DataFrame for additional processing if needed

# For .xlsx files
!pip install openpyxl

# Or for .xls files
!pip install xlwt

save_predictions_to_excel(val_image_names, val_pred_concateneated, "/content/predicted_classes.xlsx")



"""# For testing data

"""

!unzip "/content/drive/MyDrive/Testing set.zip" -d /content/

# Validation generator without balancing
test_datagen = BalancedImageDataGenerator(
    balance_classes=False,  # Disable balancing for validation
    rescale=1./255
)

test_dir = "/content/Testing set"
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    shuffle=False,
    batch_size=32,
    class_mode='categorical'
)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import balanced_accuracy_score, f1_score

# Calculate total number of batches from the generator
total_batches = len(test_generator)
print(f"Total number of batches: {total_batches}")

# Step 1: Generate predictions for all batches
test_pred = []
test_true = []
test_predictions = []
test_true_classes = []
test_image_names = []

for i, (x, y, img_names) in enumerate(test_generator):
    if i % 50 == 0:  # Print progress every 50 batches
        print(f"Processing batch {i}/{total_batches}")

    preds = resnet_model.predict(x)
    test_pred.append(preds['output_0'])
    test_predictions.append(np.argmax(preds['output_0'], axis=1))
    # test_true.append(y)
    # test_true_classes.append(np.argmax(y, axis=1))
    test_image_names.append(img_names)

    if i >= total_batches - 1:
        break

# Step 2: Concatenate predictions, true labels, and image names across batches
test_pred_concateneated = np.concatenate(test_pred)
# test_true = np.concatenate(test_true)
test_pred_classes = np.concatenate(test_predictions)
# test_true_classes = np.concatenate(test_true_classes)
test_image_names = np.concatenate(test_image_names)

print("\nFinal shapes:",
      # "\nTrue classes:", test_true_classes.shape,
      "\nPredicted classes:", test_pred_classes.shape,
      "\nImage names:",test_image_names.shape,
      # "\nTest true:", test_true.shape,
      "\nTest_pred_concatenated:", test_pred_concateneated.shape)

# Create a results dictionary mapping image names to their predictions and true labels
# results = {
#     'image_names': test_image_names,
#     'true_classes': test_true_classes,
#     'predicted_classes': test_pred_classes
# }

# Create a DataFrame for easier analysis
# import pandas as pd
# results_df = pd.DataFrame({
#     'image_name': val_image_names,
#     'true_class': val_true_classes,
#     'predicted_class': val_pred_classes,
#     'correct_prediction': val_true_classes == val_pred_classes
# })

# # Print some example results
# print("\nExample results:")
# print(results_df.head())

# # Calculate metrics
# balanced_acc = balanced_accuracy_score(val_true_classes, val_pred_classes)
# f1 = f1_score(val_true_classes, val_pred_classes, average='weighted')

# print(f"\nBalanced Accuracy: {balanced_acc:.4f}")
# print(f"F1 Score: {f1:.4f}")

# # Create confusion matrix
# cm = confusion_matrix(val_true_classes, val_pred_classes)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm)
# disp.plot(cmap=plt.cm.Blues)
# plt.title('Confusion Matrix')
# plt.show()

# # Save results to CSV
# results_df.to_csv('validation_results.csv', index=False)

save_predictions_to_excel(test_image_names, test_pred_concateneated, "/content/predicted_classes_test.xlsx")

